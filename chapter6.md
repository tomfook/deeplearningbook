## Chapter 6 Deep Feedforward Networks

ディープ・フィードフォワード・ネットワーク、フィードフォワード・ニューラル・ネットワークもしくは多層パーセプトロン(MLPs)とも呼ばれる、は典型的なディープラーニングモデルである。
フィードフォワードネットワークの目的はある関数$f^*$を近似することである。
たとえば分類機であれば$y=f^*(\mathbf x)$は入力$\mathbf xの分類$y$への写像である。
フィードフォワードネットワークは写像$\mathbf y=f(\mathbf x;\mathbf \theta)$を定義し、最も良い関数近似になるパラメータ$\mathbf \theta$の値を学習する。

これらのモデルが**フィードフォワード(feedforward)**と呼ばれるのは、$\mathbf x$により評価される関数を通して情報が中間的な計算$f$を通って最後に$\mathbf y$として出力されるためである。
モデルの出力をモデル自身に戻す**フィードバック(feedback)**接続は存在しない。
フィードフォワード・ニューラル・ネットワークがフィードバック接続を含むよう拡張されるとき、それらはリカレント・ニューラル・ネットワークと呼ばれる。
これについてはchapter10で説明する。

フィードフォワード・ネットワークは機械学習の実務家にとって最も重要である。
これらはいくつもの重要な商用アプリケーションの基礎になっている。
たとえば写真の物体認識に用いられる畳み込みネットワークはフィードフォワード・ネットワークの一種である。
フィードフォワード・ネットワークは自然言語に適用されるリカレント・ネットワークの着想の基礎になっている。

フィードフォワード・ニューラル・ネットワークが**ネットワーク**と呼ばれるのは、典型的には多くの異なる関数を繋げることによって表されるためである。
モデルは関数がどのように繋がっているかを表す有向非巡回グラフと関連付けられる。たとえば、３つの関数$f^{(1)},f^{(2)},f^{(3)}$があり、これらが鎖のように繋がって$f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$を構成しているとする。
これはニューラル・ネットワークで最もよく使われる構造である。
この場合において、$f^{(1)}$はネットワークの**第１層**と呼ばれ、$f^{(2)}$は**第２層**と呼ばれ、以下も同様である。
この鎖全体の長さがモデルの**深さ**を表す。
「ディープラーニング」という言葉はこの文脈で出てきた。
フィードフォワード・ネットワークの最後の層は**出力層**と呼ばれる。
ニューラル・ネットワークの訓練の際、$f^*(x)$に適合するよう$f(x)$を操る。
訓練データはノイズを持つため、$f^*(x)$を近似する実現値として得られる。
それぞれの実現値$x$はラベル$y\approx f^*(x)$を伴う。
訓練データは出力層がそれぞれの$x$でするべきこと、すなわち$y$に近い値を出力することを直接に決定する。
学習アルゴリズムはこれらの層が望む値を出力する方法を決定しなければならないが、訓練データはそれぞれ個々の層が何をするべきかは教えてくれない。
その代わりに学習アルゴリズムはこれらの層をどう使えば$f^*$の近似を最もよく実現するかを決定する。
訓練データはそれぞれの層の出力を示さないためこれらは**隠れ層**と呼ばれる。

